{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c7df8a7e0f5942b08aa121d9cf2f2965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d7fb8290741489093793d13e5447ff3",
              "IPY_MODEL_d85eb8c6596a464396906f0a5145a029",
              "IPY_MODEL_ee2d13f170d1407eaf6b4e762a140aa6"
            ],
            "layout": "IPY_MODEL_5a03e1df1b634f3aac9371c5aa661ef1"
          }
        },
        "0d7fb8290741489093793d13e5447ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fe6bee2c71e4932acdec0e5b37860a9",
            "placeholder": "​",
            "style": "IPY_MODEL_d837b00e707b4cfaac87f728fcc340ce",
            "value": "model.safetensors: 100%"
          }
        },
        "d85eb8c6596a464396906f0a5145a029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0036ab0e026240daa971130e14576f7a",
            "max": 2836578696,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96cecf0b3c0c4bb1bc8fe18a1a51c2ae",
            "value": 2836578696
          }
        },
        "ee2d13f170d1407eaf6b4e762a140aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edc85fad2b7b491db19e487827d17c2c",
            "placeholder": "​",
            "style": "IPY_MODEL_00c59d29308a4b24800d21de29da9845",
            "value": " 2.84G/2.84G [00:14&lt;00:00, 138MB/s]"
          }
        },
        "5a03e1df1b634f3aac9371c5aa661ef1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fe6bee2c71e4932acdec0e5b37860a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d837b00e707b4cfaac87f728fcc340ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0036ab0e026240daa971130e14576f7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cecf0b3c0c4bb1bc8fe18a1a51c2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "edc85fad2b7b491db19e487827d17c2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00c59d29308a4b24800d21de29da9845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "051dcbd2693047118046f921ca7e045b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8345772a3364108883ca581349b9c13",
              "IPY_MODEL_13b0498bc8e44ddd85ae7ec02aa3ca20",
              "IPY_MODEL_dd0397f548f844fb847381e19ad9106f"
            ],
            "layout": "IPY_MODEL_189a5b2f37084021ba0b8db89a05dfb5"
          }
        },
        "b8345772a3364108883ca581349b9c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9fbc6174bf5455386f269c091f31650",
            "placeholder": "​",
            "style": "IPY_MODEL_da3e50ebbb614f1fb04f3326aaaf6d79",
            "value": "generation_config.json: 100%"
          }
        },
        "13b0498bc8e44ddd85ae7ec02aa3ca20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98122ea43d5a424e96c41eb5712c8135",
            "max": 74,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8cb073461d54d348c5b21d22109f2a7",
            "value": 74
          }
        },
        "dd0397f548f844fb847381e19ad9106f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61b01b404b684cd4b52156d31e10a3b7",
            "placeholder": "​",
            "style": "IPY_MODEL_124a94a22f0f4bdd9cbe6035d04b6483",
            "value": " 74.0/74.0 [00:00&lt;00:00, 4.63kB/s]"
          }
        },
        "189a5b2f37084021ba0b8db89a05dfb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9fbc6174bf5455386f269c091f31650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da3e50ebbb614f1fb04f3326aaaf6d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98122ea43d5a424e96c41eb5712c8135": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8cb073461d54d348c5b21d22109f2a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61b01b404b684cd4b52156d31e10a3b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "124a94a22f0f4bdd9cbe6035d04b6483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## *Textbooks Are All You Need* Notebook Presentation\n",
        "\n",
        "#### By Jack Sun and Quentin Clark, University of Toronto\n",
        "#### For CS 2541: Large Models\n",
        "\n"
      ],
      "metadata": {
        "id": "vyX6DfotlYdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is meant to explain the main ideas behind the paper *Textbooks Are All You Need* ([see here](https://openreview.net/forum?id=Fq8tKtjACC)) with a toy example.\n",
        "\n",
        "The main thesis of this paper is that, in sequential generation tasks (i.e., for Large Language Modelling) sometimes using *higher-quality data* with a *smaller model* can lead to superior performance for downstream tasks.\n",
        "\n",
        "This notebook has two sections: one that lets you play around with the final phi-1 model from Microsoft Research, and another that walks you through a toy example with a mini-GPT model on a synthetic math dataset.\n"
      ],
      "metadata": {
        "id": "ATQDs0sKmKsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import traceback\n",
        "# import numpy as np\n",
        "# import jax\n",
        "# import jax.numpy as jnp\n",
        "# try:\n",
        "#   from penzai import pz\n",
        "# except ImportError:\n",
        "#   !pip install penzai[notebook]\n",
        "#   from penzai import pz\n",
        "# # Install necessary libraries\n",
        "# !pip install transformers torch --quiet\n",
        "# import treescope\n",
        "# from penzai.core.named_axes import wrap, nmap\n",
        "# from tqdm.notebook import tqdm\n",
        "# import matplotlib.pyplot as plt\n",
        "# import humanize\n",
        "# treescope.basic_interactive_setup(autovisualize_arrays=True)"
      ],
      "metadata": {
        "id": "0QT4OVp7jYiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDY6uQsUjZ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import required modules\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"microsoft/phi-1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n",
        "\n"
      ],
      "metadata": {
        "id": "59xtY0ARjd4l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "29ccee188dce4851ad33c17e1536a4d8",
            "1fb5893c850341c3b60f727624170634",
            "db005008233142028cf907d5625d3e2d",
            "24e3ff6017a84de6a81716c01f72da0b",
            "2f12230d08a04d589477b8cc9fffa083",
            "8a80810586d94fb38a8238ad07748e96",
            "6f21ee08e1b248da86f64b540e852c69",
            "c7df8a7e0f5942b08aa121d9cf2f2965",
            "0d7fb8290741489093793d13e5447ff3",
            "d85eb8c6596a464396906f0a5145a029",
            "ee2d13f170d1407eaf6b4e762a140aa6",
            "5a03e1df1b634f3aac9371c5aa661ef1",
            "0fe6bee2c71e4932acdec0e5b37860a9",
            "d837b00e707b4cfaac87f728fcc340ce",
            "0036ab0e026240daa971130e14576f7a",
            "96cecf0b3c0c4bb1bc8fe18a1a51c2ae",
            "edc85fad2b7b491db19e487827d17c2c",
            "00c59d29308a4b24800d21de29da9845",
            "051dcbd2693047118046f921ca7e045b",
            "b8345772a3364108883ca581349b9c13",
            "13b0498bc8e44ddd85ae7ec02aa3ca20",
            "dd0397f548f844fb847381e19ad9106f",
            "189a5b2f37084021ba0b8db89a05dfb5",
            "c9fbc6174bf5455386f269c091f31650",
            "da3e50ebbb614f1fb04f3326aaaf6d79",
            "98122ea43d5a424e96c41eb5712c8135",
            "a8cb073461d54d348c5b21d22109f2a7",
            "61b01b404b684cd4b52156d31e10a3b7",
            "124a94a22f0f4bdd9cbe6035d04b6483"
          ]
        },
        "outputId": "a3c796d9-51a2-45da-cc72-c6d910619697"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "29ccee188dce4851ad33c17e1536a4d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fb5893c850341c3b60f727624170634",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db005008233142028cf907d5625d3e2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "24e3ff6017a84de6a81716c01f72da0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f12230d08a04d589477b8cc9fffa083",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a80810586d94fb38a8238ad07748e96",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f21ee08e1b248da86f64b540e852c69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/734 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7df8a7e0f5942b08aa121d9cf2f2965"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "051dcbd2693047118046f921ca7e045b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function_text = \"\"\"def quicksort(lst: list):\n",
        "    \\\"\"\"\n",
        "    This takes a list of integers and sorts the list in ascending order.\n",
        "    \\\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "print(function_text)\n",
        "\n",
        "# Test the model\n",
        "inputs = tokenizer(function_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "output = model.generate(**inputs, max_length=200)\n",
        "\n",
        "# Decode and print the output\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Input:\", function_text)\n",
        "print(\"Output:\", decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QHWOS94kGes",
        "outputId": "2f2f144e-60a9-4913-ceaa-765c25ba0527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def quicksort(lst: list):\n",
            "    \"\"\"\n",
            "    This takes a list of integers and sorts the list in ascending order.\n",
            "    \"\"\"\n",
            "\n",
            "Input: def quicksort(lst: list):\n",
            "    \"\"\"\n",
            "    This takes a list of integers and sorts the list in ascending order.\n",
            "    \"\"\"\n",
            "\n",
            "Output: def quicksort(lst: list):\n",
            "    \"\"\"\n",
            "    This takes a list of integers and sorts the list in ascending order.\n",
            "    \"\"\"\n",
            "    if len(lst) <= 1:\n",
            "        return lst\n",
            "    else:\n",
            "        pivot = lst[0]\n",
            "        left = []\n",
            "        right = []\n",
            "        for num in lst[1:]:\n",
            "            if num < pivot:\n",
            "                left.append(num)\n",
            "            else:\n",
            "                right.append(num)\n",
            "        return quicksort(left) + [pivot] + quicksort(right)\n",
            "\n",
            "\n",
            "\n",
            "from typing import List\n",
            "\n",
            "def find_smallest_multiple_of_list(li: List[int]) -> int:\n",
            "    \"\"\"\n",
            "    Returns the smallest positive integer that is divisible by all the numbers in the input list.\n",
            "\n",
            "    Args:\n",
            "    li (List[int]): A list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def quicksort(lst: list):\n",
        "    \"\"\"\n",
        "    This takes a list of integers and sorts the list in ascending order.\n",
        "    \"\"\"\n",
        "    if len(lst) <= 1:\n",
        "        return lst\n",
        "    else:\n",
        "        pivot = lst[0]\n",
        "        left = []\n",
        "        right = []\n",
        "        for i in range(1, len(lst)):\n",
        "            if lst[i] < pivot:\n",
        "                left.append(lst[i])\n",
        "            else:\n",
        "                right.append(lst[i])\n",
        "        return quicksort(left) + [pivot] + quicksort(right)\n",
        "lst = quicksort([3,2,1])\n",
        "print(lst)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXH8cKe2lWCv",
        "outputId": "bf18d28f-689e-45c9-ba0a-0ebce6587d0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "function_text = \"\"\"def sort_concat_square_deduplicate(list1, list2, my_threshold):\n",
        "    \\\"\"\"\n",
        "    This function takes two lists of integers, sorts each of them in ascending order,\n",
        "    concatenates them, squares the entries at even indices, filters out entries\n",
        "    smaller than my_threshold and then removes duplicates. The resulting list is\n",
        "    returned.\n",
        "    \\\"\"\"\n",
        "\"\"\"\n",
        "print(function_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZDL1iJLb55F",
        "outputId": "c216268b-c509-42c2-b07a-c445de678a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def sort_concat_square_deduplicate(list1, list2, my_threshold):\n",
            "    \"\"\"\n",
            "    This function takes two lists of integers, sorts each of them in ascending order,\n",
            "    concatenates them, squares the entries at even indices, filters out entries\n",
            "    smaller than my_threshold and then removes duplicates. The resulting list is\n",
            "    returned.\n",
            "    \"\"\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "inputs = tokenizer(function_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "output = model.generate(**inputs, max_length=200)\n",
        "\n",
        "# Decode and print the output\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Input:\", function_text)\n",
        "print(\"Output:\", decoded_output)"
      ],
      "metadata": {
        "id": "3sw1NjgykpHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2456ace4-6684-4303-aed8-cbc37fe9ba83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: def sort_concat_square_deduplicate(list1, list2, my_threshold):\n",
            "    \"\"\"\n",
            "    This function takes two lists of integers, sorts each of them in ascending order,\n",
            "    concatenates them, squares the entries at even indices, filters out entries\n",
            "    smaller than my_threshold and then removes duplicates. The resulting list is\n",
            "    returned.\n",
            "    \"\"\"\n",
            "\n",
            "Output: def sort_concat_square_deduplicate(list1, list2, my_threshold):\n",
            "    \"\"\"\n",
            "    This function takes two lists of integers, sorts each of them in ascending order,\n",
            "    concatenates them, squares the entries at even indices, filters out entries\n",
            "    smaller than my_threshold and then removes duplicates. The resulting list is\n",
            "    returned.\n",
            "    \"\"\"\n",
            "    # Sort and concatenate the two lists\n",
            "    combined_list = sorted(list1 + list2)\n",
            "    \n",
            "    # Square the entries at even indices\n",
            "    squared_list = [num**2 for i, num in enumerate(combined_list) if i % 2 == 0]\n",
            "    \n",
            "    # Filter out entries smaller than my_threshold and remove duplicates\n",
            "    filtered_list = list(set(squared_list))\n",
            "    \n",
            "    return filtered_list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# phi-1-generated code\n",
        "def sort_concat_square_deduplicate(list1, list2, my_threshold):\n",
        "    \"\"\"\n",
        "    This function takes two lists of integers, sorts each of them in ascending order,\n",
        "    concatenates them, squares the entries at even indices, filters out entries\n",
        "    smaller than my_threshold and then removes duplicates. The resulting list is\n",
        "    returned.\n",
        "    \"\"\"\n",
        "    # Sort and concatenate the two lists\n",
        "    combined_list = sorted(list1 + list2)\n",
        "\n",
        "    # Square the entries at even indices\n",
        "    squared_list = [num**2 for i, num in enumerate(combined_list) if i % 2 == 0]\n",
        "\n",
        "    # Filter out entries smaller than my_threshold and remove duplicates\n",
        "    filtered_list = list(set(squared_list))\n",
        "\n",
        "    return filtered_list\n",
        "\n",
        "# Almost correct, but failed to filter out entries smaller than my_threshold\n",
        "test_list = [1,2,3,4,5]\n",
        "test_list2 = [6,7,8,9,10]\n",
        "result = sort_concat_square_deduplicate(test_list, test_list2, 5)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-CYJ7zSiCW0",
        "outputId": "85d0f011-384b-4c4a-af42-8c5d0b0d289f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 9, 81, 49, 25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HumanEval:\n",
        "function_text = \"\"\"You are given a non-empty list of positive\n",
        "integers. Return the greatest integer that\n",
        "is greater than zero, and has a frequency\n",
        "greater than or equal to the value of the\n",
        "integer itself. The frequency of an integer\n",
        "is the number of times it appears in the list.\"\"\""
      ],
      "metadata": {
        "id": "zIaS9hRedpmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "inputs = tokenizer(function_text, return_tensors=\"pt\")\n",
        "\n",
        "# Generate output\n",
        "output = model.generate(**inputs, max_length=400)\n",
        "\n",
        "# Decode and print the output\n",
        "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Input:\", function_text)\n",
        "print(\"Output:\", decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_9F02qGeiAD",
        "outputId": "92f9a54a-86a4-49d2-cc66-a5e12a8e4ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: You are given a non-empty list of positive\n",
            "integers. Return the greatest integer that\n",
            "is greater than zero, and has a frequency\n",
            "greater than or equal to the value of the\n",
            "integer itself. The frequency of an integer\n",
            "is the number of times it appears in the list.\n",
            "Output: You are given a non-empty list of positive\n",
            "integers. Return the greatest integer that\n",
            "is greater than zero, and has a frequency\n",
            "greater than or equal to the value of the\n",
            "integer itself. The frequency of an integer\n",
            "is the number of times it appears in the list.\n",
            "\n",
            "Example:\n",
            "greatest_frequency_above_zero([1, 2, 2, 3, 3, 3, 4, 4, 4, 4]) -> 4\n",
            "greatest_frequency_above_zero([0, 0, 0, 0, 0]) -> 0\n",
            "greatest_frequency_above_zero([1, 1, 1, 1, 1]) -> 1\n",
            "\"\"\"\n",
            "\n",
            "from typing import List\n",
            "\n",
            "def greatest_frequency_above_zero(li: List[int]) -> int:\n",
            "    \"\"\"\n",
            "    Returns the greatest integer that is greater than zero, and has a frequency\n",
            "    greater than or equal to the value of the integer itself.\n",
            "    \n",
            "    Args:\n",
            "    li: A list of positive integers\n",
            "    \n",
            "    Returns:\n",
            "    The greatest integer that is greater than zero, and has a frequency greater\n",
            "    than or equal to the value of the integer itself. If no such integer exists,\n",
            "    returns 0.\n",
            "    \"\"\"\n",
            "    freq = {}\n",
            "    for num in li:\n",
            "        if num > 0:\n",
            "            freq[num] = freq.get(num, 0) + 1\n",
            "    max_freq = 0\n",
            "    max_num = 0\n",
            "    for num, count in freq.items():\n",
            "        if num >= count and num > max_num:\n",
            "            max_freq = count\n",
            "            max_num = num\n",
            "    return max_num\n",
            "\n",
            "\n",
            "\n",
            "from typing import List\n",
            "\n",
            "def sum_of_squares_minus_mean(li: List[int]) -> int:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "def greatest_frequency_above_zero(li: List[int]) -> int:\n",
        "    \"\"\"\n",
        "    Returns the greatest integer that is greater than zero, and has a frequency\n",
        "    greater than or equal to the value of the integer itself.\n",
        "\n",
        "    Args:\n",
        "    li: A list of positive integers\n",
        "\n",
        "    Returns:\n",
        "    The greatest integer that is greater than zero, and has a frequency greater\n",
        "    than or equal to the value of the integer itself. If no such integer exists,\n",
        "    returns 0.\n",
        "    \"\"\"\n",
        "    freq = {}\n",
        "    for num in li:\n",
        "        if num > 0:\n",
        "            freq[num] = freq.get(num, 0) + 1\n",
        "    max_freq = 0\n",
        "    max_num = 0\n",
        "    for num, count in freq.items():\n",
        "        if num >= count and num > max_num:\n",
        "            max_freq = count\n",
        "            max_num = num\n",
        "    return max_num\n",
        "print(greatest_frequency_above_zero([1, 2, 2, 3, 3, 3, 4, 4, 4, 4,5])) # Cannot filter out the number with frequence smaller than it self. Should return 4 instead.\n",
        "print(greatest_frequency_above_zero([0, 0, 0, 0, 0]))"
      ],
      "metadata": {
        "id": "sXsy7p-yj9WV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9be529-aab6-48b8-cccb-ca854c8af6c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2 - Math-GPT\n",
        "\n",
        "To demonstrate the main result of the *Texbooks* paper in a toy environment, we will construct our own small GPT model trained to perform basic arithmetic.\n",
        "\n",
        "We will make two models - a larger one trained on a larger amount of low-quality data (many of the examples will be incorrect) and a smaller one trained on a smaller amount of high-quality data (all of the examples will be correct).\n"
      ],
      "metadata": {
        "id": "hKB_BW-IEbuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ## Basic imports\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "4mYoLSbgCXBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define our dataset, which can either be high quality (have no mistakes, smaller size) or low quality (have mistakes ~1/3rd of the time, larger size.)"
      ],
      "metadata": {
        "id": "x-rbpJ1GRzSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AdditionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Define the addition dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ndigit, split, quality = 'High'):\n",
        "        self.quality = quality\n",
        "        self.split = split # train/test\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
        "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
        "\n",
        "        # split up all addition problems into either training data or test data\n",
        "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
        "        r = np.random.RandomState(1337) # make deterministic\n",
        "        perm = r.permutation(num)\n",
        "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
        "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
        "        if quality == 'High' and split != 'test': # makes dataset smaller\n",
        "          self.len = int(self.ixes.size/5)\n",
        "          self.ixes = self.ixes[:self.len]\n",
        "        elif quality == 'Low' or split == 'test': # makes dataset larger\n",
        "          self.len = self.ixes.size\n",
        "        else:\n",
        "          print('ERROR: Quality Not Defined!')\n",
        "    def __len__(self):\n",
        "        return self.ixes.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # given a problem index idx, first recover the associated a + b\n",
        "        idx = self.ixes[idx]\n",
        "        nd = 10**self.ndigit\n",
        "        a = idx // nd\n",
        "        b = idx %  nd\n",
        "        c = a + b\n",
        "        # if low quality, randomly introduce a mistake to the answer\n",
        "        if self.quality == 'Low' and self.split == 'train':\n",
        "          r = np.random.RandomState(idx)\n",
        "          if np.random.RandomState(idx).rand() < 0.33: # third of the time give a wrong answer by adding a random value from [-5,5] to the answer\n",
        "            c = c + np.random.RandomState(idx).randint(-5,5)\n",
        "            c = max(0,c)\n",
        "            #print(c)\n",
        "\n",
        "\n",
        "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\"\n",
        "        dix = [int(s) for s in render] # convert each character to its token index\n",
        "        # x will be input to GPT and y will be the associated expected outputs\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
        "        y[:self.ndigit*2-1] = -100\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "E1304hO-mksZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataset for e.g. 2-digit addition\n",
        "ndigit = 2\n",
        "train_dataset_high_quality = AdditionDataset(ndigit=ndigit, split='train', quality = 'High')\n",
        "train_dataset_low_quality = AdditionDataset(ndigit=ndigit, split='train', quality = 'Low')\n",
        "test_dataset = AdditionDataset(ndigit=ndigit, split='test')\n",
        "\n",
        "print('High-Quality Training Set Size:',train_dataset_high_quality.len)\n",
        "print('Low-Quality Training Set Size:',train_dataset_low_quality.len)\n",
        "print('Test Set Size:',test_dataset.len)"
      ],
      "metadata": {
        "id": "qIkyGma1ns5E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bd56341-1203-4362-87ac-24d6d356aadc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High-Quality Training Set Size: 1800\n",
            "Low-Quality Training Set Size: 9000\n",
            "Test Set Size: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As promised, our high-quality dataset is a fifth the size of our low quality one. We also defined a holdout dataset.\n",
        "\n",
        "The next cell is hidden for brevity, but it defines our Transformer architecture as a pretty standard GPT-2 style decoder-only model with multihead attention."
      ],
      "metadata": {
        "id": "772naay6SDEn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Architectue Definiton (unimportant)\n",
        "class GPT(nn.Module):\n",
        "    \"\"\"  the full GPT language model, with a squence size of block_size \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, n_embd, n_head, block_size, n_layer, embd_pdrop=0.1, attn_pdrop=0.1,resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, block_size, n_embd))\n",
        "        self.drop = nn.Dropout(embd_pdrop)\n",
        "        # transformer\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head, block_size, attn_pdrop, resid_pdrop)\n",
        "                                      for _ in range(n_layer)])\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        logger.info(\"number of parameters: %e\", sum(p.numel() for p in self.parameters()))\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "        \"\"\"\n",
        "        You don't need to change this function. This is setting specific parameters for optimization.\n",
        "        \"\"\"\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        # special case the position embedding parameter in the root GPT module as not decayed\n",
        "        no_decay.add('pos_emb')\n",
        "\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
        "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
        "                                                    % (str(param_dict.keys() - union_params), )\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        b, t = x.size()\n",
        "        assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "\n",
        "        # forward the GPT model\n",
        "        token_embeddings=self.tok_emb(x)\n",
        "        position_embeddings=self.pos_emb[:,:t,:]\n",
        "        x=self.drop(token_embeddings+position_embeddings)\n",
        "        x=self.blocks(x)\n",
        "        x=self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "          loss=F.cross_entropy(logits.view(-1,logits.size(-1)),targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\" an Transformer block \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadSelfAttention(n_embd, n_head, block_size, attn_pdrop, resid_pdrop)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(resid_pdrop),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"YOUR CODE HERE?\"\"\"\n",
        "        x=x+self.attn(self.ln1(x))\n",
        "        x=x+self.mlp(self.ln2(x))\n",
        "        return x\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    You can also use torch.nn.MultiheadAttention to validate your implementation\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head, block_size, attn_pdrop=0.1, resid_pdrop=0.1):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        #Define key, query, value projections for all heads\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        self.key = nn.Linear(n_embd,n_embd)\n",
        "        self.query = nn.Linear(n_embd,n_embd)\n",
        "        self.value = nn.Linear(n_embd,n_embd)\n",
        "        # Dropout layers\n",
        "        self.attn_drop = nn.Dropout(attn_pdrop)\n",
        "        self.resid_drop = nn.Dropout(resid_pdrop)\n",
        "        # output projection\n",
        "        self.proj = nn.Linear(n_embd,n_embd)\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        #self.mask = torch.tril(torch.ones(block_size,block_size)).view(1,1,block_size,block_size)\n",
        "        self.register_buffer(\"mask\",torch.tril(torch.ones(block_size,block_size)).view(1,1,block_size,block_size))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, layer_past=None):\n",
        "        B, T, C = x.size() # B = Batch\n",
        "        \"\"\"YOUR CODE HERE\"\"\"\n",
        "        k=self.key(x).view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "        q=self.query(x).view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "        v=self.value(x).view(B,T,self.n_head,C//self.n_head).transpose(1,2)\n",
        "\n",
        "        #\n",
        "        att=(q @ k.transpose(-2,-1))*(1.0/math.sqrt(k.size(-1)))\n",
        "        att=att.masked_fill(self.mask[:,:,:T,:T]==0,float('-inf'))\n",
        "        att=F.softmax(att,dim=-1)\n",
        "        att=self.attn_drop(att)\n",
        "        y=att @ v\n",
        "        y=y.transpose(1,2).contiguous().view(B,T,C)\n",
        "        output=self.resid_drop(self.proj(y))\n",
        "\n",
        "        return output\n",
        "import math\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "2vSXY5ktnwZ5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we instantiate a large and small version of our GPT model, and define some training parameters."
      ],
      "metadata": {
        "id": "feFvH8QMStRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "class TrainerConfig:\n",
        "    # optimization parameters\n",
        "    max_epochs = 10\n",
        "    batch_size = 64\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    grad_norm_clip = 1.0\n",
        "    weight_decay = 0.1 # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    lr_decay = False\n",
        "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "    # checkpoint settings\n",
        "    ckpt_path = None\n",
        "    num_workers = 0 # for DataLoader\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k,v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "# initialize a baby GPT model\n",
        "model_high_quality = GPT(vocab_size = train_dataset_high_quality.vocab_size, n_embd=128, n_head=4, block_size =  train_dataset_high_quality.block_size, n_layer=2)\n",
        "model_low_quality = GPT(vocab_size = train_dataset_high_quality.vocab_size, n_embd=248, n_head=4, block_size =  train_dataset_high_quality.block_size, n_layer=4)\n",
        "\n",
        "print('Model Size: High-Quality Model: ',sum(p.numel() for p in model_high_quality.parameters()))\n",
        "print('Model Size:  Low-Quality Model:',sum(p.numel() for p in model_low_quality.parameters()))"
      ],
      "metadata": {
        "id": "repeS2C6ny6I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6c16a2-73f3-4818-acf7-746591c176a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size: High-Quality Model:  400128\n",
            "Model Size:  Low-Quality Model: 2972032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like in the Textbooks paper, our model that we will give high-quality data is much smaller (~6 times) smaller than the large model.\n",
        "\n",
        "Next, we define some boiler plate training code (we've also hidden this for ease of reading). It is standard stuff you will see in any PyTorch training loop. One note - we increase the number of epochs for the high-quality model, so it is seeing roughly the same number of gradient updates/wall-clock trianing time as the low-quality model. We need to do this because the high-quality model has less data, so the same number of epochs will lead to less gradient steps."
      ],
      "metadata": {
        "id": "grEgIOxXS1sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Loop\n",
        "config_high_quality = TrainerConfig(max_epochs=250, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset_high_quality)*(ndigit+1),\n",
        "                      num_workers=4)\n",
        "\n",
        "config_low_quality = TrainerConfig(max_epochs=50, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset_low_quality)*(ndigit+1),\n",
        "                      num_workers=4)\n",
        "from tqdm import trange\n",
        "def train_gpt(config,model,dataset):\n",
        "  device = 'cpu'\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    model.to(device)\n",
        "  optimizer = model.configure_optimizers(config)\n",
        "\n",
        "\n",
        "\n",
        "  tokens = 0\n",
        "  for epoch in trange(config.max_epochs):\n",
        "      model.train()\n",
        "      data = dataset\n",
        "      loader = DataLoader(data, shuffle=True, pin_memory=True,\n",
        "                          batch_size=config.batch_size,\n",
        "                          num_workers=config.num_workers)\n",
        "      losses = []\n",
        "      #pbar = tqdm(enumerate(loader), total=len(loader))\n",
        "      for iter, (x, y) in enumerate(loader):\n",
        "          # place data on the correct device\n",
        "          x = x.to(device)\n",
        "          y = y.to(device)\n",
        "          # forward the model\n",
        "          logits, loss = model(x, y)\n",
        "          loss = loss.mean()\n",
        "          model.zero_grad()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "          optimizer.step()\n",
        "          # decay the learning rate based on our progress\n",
        "          if config.lr_decay:\n",
        "              tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "              if tokens < config.warmup_tokens:\n",
        "                  # linear warmup\n",
        "                  lr_mult = float(tokens) / float(max(1, config.warmup_tokens))\n",
        "              else:\n",
        "                  # cosine learning rate decay\n",
        "                  progress = float(tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
        "                  lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "              lr = config.learning_rate * lr_mult\n",
        "              for param_group in optimizer.param_groups:\n",
        "                  param_group['lr'] = lr\n",
        "          else:\n",
        "              lr = config.learning_rate\n",
        "          # report progress\n",
        "          #pbar.set_description(f\"epoch {epoch+1} iter {iter}: train loss {loss.item():.5f}. lr {lr:e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gNtTry17n3I5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can train our model. This will take a little while, so be patient!"
      ],
      "metadata": {
        "id": "1NS7ohMgTxOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_gpt(config_high_quality,model_high_quality,train_dataset_high_quality)\n",
        "train_gpt(config_low_quality,model_low_quality,train_dataset_low_quality)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCqzl7vJTW1b",
        "outputId": "0c1bec35-ea87-4a73-e4d5-6e52549c99eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 250/250 [01:06<00:00,  3.75it/s]\n",
            "100%|██████████| 50/50 [04:21<00:00,  5.24s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost done! Now it is time to see how our models perform on our smaller test datset. The next cell defines some basic sampling utilities. For the final time, we condense them for brevity."
      ],
      "metadata": {
        "id": "Z0TORcroT8DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sampling Utilities\n",
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "def sample(train_dataset,model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time.\n",
        "    \"\"\"\n",
        "    block_size = train_dataset.block_size\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        logits, _ = model(x_cond)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "    return x\n",
        "def Addition_GPT(model,dataset, batch_size=32, max_batches=-1):\n",
        "    device = 'cpu'\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.cuda.current_device()\n",
        "    results = []\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    for b, (x, y) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        d1d2 = x[:, :ndigit*2]\n",
        "        d1d2d3 = sample(dataset,model, d1d2, ndigit+1)\n",
        "        d3 = d1d2d3[:, -(ndigit+1):]\n",
        "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(device)\n",
        "        # decode the integers from individual digits\n",
        "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
        "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
        "        d3i_pred = (d3 * factors).sum(1)\n",
        "        d3i_gt = d1i + d2i\n",
        "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
        "        for i in range(x.size(0)):\n",
        "            results.append(int(correct[i]))\n",
        "            judge = 'CORRECT' if correct[i] else 'WRONG'\n",
        "            if not correct[i]:\n",
        "                #print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\"\n",
        "                      #% (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
        "                 meow = 5\n",
        "\n",
        "        if max_batches >= 0 and b+1 >= max_batches:\n",
        "            break\n",
        "\n",
        "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
      ],
      "metadata": {
        "id": "FPbY1uY1n6SK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, it is time to see the results and run each of our models on the test dataset:"
      ],
      "metadata": {
        "id": "1cWl7WQHUITe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('High-Quality Model Results:')\n",
        "Addition_GPT(model_high_quality,test_dataset, batch_size=1024, max_batches=10)\n",
        "print('Low-Quality Model Results:')\n",
        "Addition_GPT(model_low_quality,test_dataset, batch_size=1024, max_batches=10)"
      ],
      "metadata": {
        "id": "Y9Kj50VgoAz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "590fffe6-a04e-4db3-cf07-e51b05ff18b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High-Quality Model Results:\n",
            "final score: 998/1000 = 99.80% correct\n",
            "Low-Quality Model Results:\n",
            "final score: 563/1000 = 56.30% correct\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, our model trained on a larger dataset with a significant amount of mistakes performs much worse than our model trained with a smaller, curated set of examples!"
      ],
      "metadata": {
        "id": "QRHEcrNqWEbi"
      }
    }
  ]
}