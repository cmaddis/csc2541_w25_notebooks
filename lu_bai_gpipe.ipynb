{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super(ToyModel, self).__init__()\n        self.net1 = torch.nn.Linear(10, 10).to('cuda:0')\n        self.relu = torch.nn.ReLU()\n        self.net2 = torch.nn.Linear(10, 5).to('cuda:1')\n\n    def forward(self, x):\n        x = self.relu(self.net1(x.to('cuda:0')))\n        return self.net2(x.to('cuda:1'))","metadata":{"execution":{"iopub.execute_input":"2025-01-30T04:27:59.494240Z","iopub.status.busy":"2025-01-30T04:27:59.493908Z","iopub.status.idle":"2025-01-30T04:27:59.500191Z","shell.execute_reply":"2025-01-30T04:27:59.499392Z","shell.execute_reply.started":"2025-01-30T04:27:59.494211Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"model = ToyModel()\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\noptimizer.zero_grad()\noutputs = model(torch.randn(20, 10))\nlabels = torch.randn(20, 5).to('cuda:1')\nloss_fn(outputs, labels).backward()\noptimizer.step()","metadata":{"execution":{"iopub.execute_input":"2025-01-30T04:27:59.501752Z","iopub.status.busy":"2025-01-30T04:27:59.501467Z","iopub.status.idle":"2025-01-30T04:27:59.525788Z","shell.execute_reply":"2025-01-30T04:27:59.524950Z","shell.execute_reply.started":"2025-01-30T04:27:59.501724Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torchvision.models.resnet import ResNet, Bottleneck\n\nnum_classes = 1000\n\n\nclass ModelParallelResNet50(ResNet):\n    def __init__(self, *args, **kwargs):\n        super(ModelParallelResNet50, self).__init__(\n            Bottleneck, [3, 4, 6, 3], num_classes=num_classes, *args, **kwargs)\n\n        self.seq1 = nn.Sequential(\n            self.conv1,\n            self.bn1,\n            self.relu,\n            self.maxpool,\n\n            self.layer1,\n            self.layer2\n        ).to('cuda:0')\n\n        self.seq2 = nn.Sequential(\n            self.layer3,\n            self.layer4,\n            self.avgpool,\n        ).to('cuda:1')\n\n        self.fc.to('cuda:1')\n\n    def forward(self, x):\n        x = self.seq2(self.seq1(x).to('cuda:1'))\n        return self.fc(x.view(x.size(0), -1))","metadata":{"execution":{"iopub.execute_input":"2025-01-30T04:27:59.527405Z","iopub.status.busy":"2025-01-30T04:27:59.527164Z","iopub.status.idle":"2025-01-30T04:27:59.540620Z","shell.execute_reply":"2025-01-30T04:27:59.539969Z","shell.execute_reply.started":"2025-01-30T04:27:59.527387Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torchvision.models as models\n\nnum_batches = 3\nbatch_size = 120\nimage_w = 128\nimage_h = 128\n\n\ndef train(model):\n    model.train(True)\n    loss_fn = nn.MSELoss()\n    optimizer = optim.SGD(model.parameters(), lr=0.001)\n\n    one_hot_indices = torch.LongTensor(batch_size) \\\n                           .random_(0, num_classes) \\\n                           .view(batch_size, 1)\n\n    for _ in range(num_batches):\n        # generate random inputs and labels\n        inputs = torch.randn(batch_size, 3, image_w, image_h)\n        labels = torch.zeros(batch_size, num_classes) \\\n                      .scatter_(1, one_hot_indices, 1)\n\n        # run forward pass\n        optimizer.zero_grad()\n        outputs = model(inputs.to('cuda:0'))\n\n        # run backward pass\n        labels = labels.to(outputs.device)\n        loss_fn(outputs, labels).backward()\n        optimizer.step()","metadata":{"execution":{"iopub.execute_input":"2025-01-30T04:27:59.542209Z","iopub.status.busy":"2025-01-30T04:27:59.541940Z","iopub.status.idle":"2025-01-30T04:27:59.559751Z","shell.execute_reply":"2025-01-30T04:27:59.559163Z","shell.execute_reply.started":"2025-01-30T04:27:59.542189Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.switch_backend('Agg')\nimport numpy as np\nimport timeit\n\nnum_repeat = 10\n\nstmt = \"train(model)\"\n\nsetup = \"model = ModelParallelResNet50()\"\nmp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\nsetup = \"import torchvision.models as models;\" + \\\n        \"model = models.resnet50(num_classes=num_classes).to('cuda:0')\"\nrn_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\n\ndef plot(means, stds, labels, fig_name):\n    plt.style.use('ggplot')\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds,\n           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('ResNet50 Execution Time (Second)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels(labels)\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\n\nplot([mp_mean, rn_mean],\n     [mp_std, rn_std],\n     ['Naive Model Parallel', 'Single GPU'],\n     'mp_vs_rn.png')\n","metadata":{"execution":{"iopub.execute_input":"2025-01-30T04:27:59.560713Z","iopub.status.busy":"2025-01-30T04:27:59.560460Z","iopub.status.idle":"2025-01-30T04:28:25.958089Z","shell.execute_reply":"2025-01-30T04:28:25.957157Z","shell.execute_reply.started":"2025-01-30T04:27:59.560683Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class PipelineParallelResNet50(ModelParallelResNet50):\n    def __init__(self, split_size=20, *args, **kwargs):\n        super(PipelineParallelResNet50, self).__init__(*args, **kwargs)\n        self.split_size = split_size\n\n    def forward(self, x):\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.seq1(s_next).to('cuda:1')\n        ret = []\n\n        for s_next in splits:\n            # A. ``s_prev`` runs on ``cuda:1``\n            s_prev = self.seq2(s_prev)\n            ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n            # B. ``s_next`` runs on ``cuda:0``, which can run concurrently with A\n            s_prev = self.seq1(s_next).to('cuda:1')\n\n        s_prev = self.seq2(s_prev)\n        ret.append(self.fc(s_prev.view(s_prev.size(0), -1)))\n\n        return torch.cat(ret)\n\n\nsetup = \"model = PipelineParallelResNet50()\"\npp_run_times = timeit.repeat(\n    stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean],\n     [mp_std, rn_std, pp_std],\n     ['Naive Model Parallel', 'Single GPU', 'Pipelining Model Parallel'],\n     'mp_vs_rn_vs_pp.png')\n","metadata":{"execution":{"iopub.execute_input":"2025-01-30T04:28:25.959331Z","iopub.status.busy":"2025-01-30T04:28:25.959008Z","iopub.status.idle":"2025-01-30T04:28:37.218132Z","shell.execute_reply":"2025-01-30T04:28:37.217412Z","shell.execute_reply.started":"2025-01-30T04:28:25.959281Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"means = []\nstds = []\nsplit_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60]\n\nfor split_size in split_sizes:\n    setup = \"model = PipelineParallelResNet50(split_size=%d)\" % split_size\n    pp_run_times = timeit.repeat(\n        stmt, setup, number=1, repeat=num_repeat, globals=globals())\n    means.append(np.mean(pp_run_times))\n    stds.append(np.std(pp_run_times))\n    \nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nax.plot(split_sizes, means)\nax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro')\nax.set_ylabel('ResNet50 Execution Time (Second)')\nax.set_xlabel('Pipeline Split Size')\nax.set_xticks(split_sizes)\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.savefig(\"split_size_tradeoff.png\")\nplt.close(fig)\n\n","metadata":{"execution":{"iopub.execute_input":"2025-01-30T04:28:37.219237Z","iopub.status.busy":"2025-01-30T04:28:37.218953Z","iopub.status.idle":"2025-01-30T04:31:39.371624Z","shell.execute_reply":"2025-01-30T04:31:39.370629Z","shell.execute_reply.started":"2025-01-30T04:28:37.219213Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"plt.style.use('ggplot')\nfig, ax = plt.subplots()\nax.plot(split_sizes, means)\nax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro')\nax.set_ylabel('ResNet50 Execution Time (Second)')\nax.set_xlabel('Pipeline Split Size')\nax.set_xticks(split_sizes)\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.savefig(\"split_size_tradeoff.png\")\nplt.close(fig)","metadata":{"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport timeit\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# 超参数\nnum_tokens = 10000  # 词汇表大小\nembed_dim = 512\nnum_heads = 8\nnum_layers = 6\nnum_classes = 10\nseq_length = 512\nbatch_size = 32 # 128\nnum_batches = 3\nnum_repeat = 10\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-30T16:22:58.445147Z","iopub.execute_input":"2025-01-30T16:22:58.445592Z","iopub.status.idle":"2025-01-30T16:22:58.453029Z","shell.execute_reply.started":"2025-01-30T16:22:58.445556Z","shell.execute_reply":"2025-01-30T16:22:58.451642Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class ModelParallelTransformer(nn.Module):\n    def __init__(self, num_tokens=10000, embed_dim=512, num_heads=8, num_layers=6, num_classes=10):\n        super(ModelParallelTransformer, self).__init__()\n\n        self.embedding = nn.Embedding(num_tokens, embed_dim).to('cuda:0')\n        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embed_dim)).to('cuda:0')\n\n        encoder_layer1 = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads).to('cuda:0')\n        encoder_layer2 = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads).to('cuda:1')\n        \n        self.encoder1 = nn.TransformerEncoder(encoder_layer1, num_layers=num_layers // 2).to('cuda:0')\n        self.encoder2 = nn.TransformerEncoder(encoder_layer2, num_layers=num_layers // 2).to('cuda:1')\n\n        self.fc = nn.Linear(embed_dim, num_classes).to('cuda:1')\n\n    def forward(self, x):\n        x = x.to('cuda:0')\n        x = x.long()\n        \n        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n        \n        x = self.encoder1(x)\n\n        x = x.to('cuda:1')\n        x = self.encoder2(x)\n\n        x = x.mean(dim=1)\n        return self.fc(x)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-30T16:22:59.851752Z","iopub.execute_input":"2025-01-30T16:22:59.852156Z","iopub.status.idle":"2025-01-30T16:22:59.863714Z","shell.execute_reply.started":"2025-01-30T16:22:59.852123Z","shell.execute_reply":"2025-01-30T16:22:59.859927Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class VanillaTransformer(nn.Module):\n    def __init__(self, num_tokens=10000, embed_dim=512, num_heads=8, num_layers=6, num_classes=10):\n        super(VanillaTransformer, self).__init__()\n\n        self.embedding = nn.Embedding(num_tokens, embed_dim).to('cuda:0')\n        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embed_dim)).to('cuda:0')\n\n        encoder_layer1 = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads).to('cuda:0')\n        encoder_layer2 = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads).to('cuda:0')\n        \n        self.encoder1 = nn.TransformerEncoder(encoder_layer1, num_layers=num_layers // 2).to('cuda:0')\n        self.encoder2 = nn.TransformerEncoder(encoder_layer2, num_layers=num_layers // 2).to('cuda:0')\n\n        self.fc = nn.Linear(embed_dim, num_classes).to('cuda:0')\n\n    def forward(self, x):\n        x = x.to('cuda:0')\n        x = x.long()\n\n        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n        \n        x = self.encoder1(x)\n\n        x = self.encoder2(x)\n\n        x = x.mean(dim=1)\n        return self.fc(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T16:23:01.459631Z","iopub.execute_input":"2025-01-30T16:23:01.460051Z","iopub.status.idle":"2025-01-30T16:23:01.467650Z","shell.execute_reply.started":"2025-01-30T16:23:01.460017Z","shell.execute_reply":"2025-01-30T16:23:01.466608Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def train(model):\n    model.train()\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    for _ in range(num_batches):\n        inputs = torch.rand(batch_size, seq_length).to('cuda:0') * num_tokens\n        labels = torch.rand(batch_size,).to('cuda:1').type(torch.LongTensor)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n\n        labels = labels.to(outputs.device)\n        loss_fn(outputs, labels).backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2025-01-30T16:23:02.901785Z","iopub.execute_input":"2025-01-30T16:23:02.902173Z","iopub.status.idle":"2025-01-30T16:23:02.908197Z","shell.execute_reply.started":"2025-01-30T16:23:02.902145Z","shell.execute_reply":"2025-01-30T16:23:02.907012Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"setup = \"model = ModelParallelTransformer(num_tokens)\"\nstmt = \"train(model)\"\n\nmp_run_times = timeit.repeat(stmt, setup, number=1, repeat=num_repeat, globals=globals())\nmp_mean, mp_std = np.mean(mp_run_times), np.std(mp_run_times)\n\ntorch.cuda.empty_cache()\n\nsetup = \"model = VanillaTransformer(num_tokens)\"\nrn_run_times = timeit.repeat(stmt, setup, number=1, repeat=num_repeat, globals=globals())\nrn_mean, rn_std = np.mean(rn_run_times), np.std(rn_run_times)\n\ndef plot(means, stds, labels, fig_name):\n    plt.style.use('ggplot')\n    fig, ax = plt.subplots()\n    ax.bar(np.arange(len(means)), means, yerr=stds, align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n    ax.set_ylabel('Transformer Execution Time (Seconds)')\n    ax.set_xticks(np.arange(len(means)))\n    ax.set_xticklabels(labels)\n    ax.yaxis.grid(True)\n    plt.tight_layout()\n    plt.savefig(fig_name)\n    plt.close(fig)\n\nplot([mp_mean, rn_mean], [mp_std, rn_std], ['Model Parallel', 'Single GPU'], 'mp_vs_rn.png')\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-30T16:23:04.218573Z","iopub.execute_input":"2025-01-30T16:23:04.218964Z","iopub.status.idle":"2025-01-30T16:23:52.231000Z","shell.execute_reply.started":"2025-01-30T16:23:04.218920Z","shell.execute_reply":"2025-01-30T16:23:52.230003Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class PipelineParallelTransformer(ModelParallelTransformer):\n    # def __init__(self, split_size=20, *args, **kwargs):\n    def __init__(self, split_size, *args, **kwargs):\n        super().__init__()\n        self.split_size = split_size\n\n    def forward(self, x):\n        x = torch.tensor(x).to(torch.int64)\n        splits = iter(x.split(self.split_size, dim=0))\n        s_next = next(splits)\n        s_prev = self.encoder1(self.embedding(s_next.to('cuda:0')) + self.positional_encoding[:, :s_next.size(1), :]).to('cuda:1')\n\n        ret = []\n        for s_next in splits:\n            s_prev = self.encoder2(s_prev)\n            ret.append(self.fc(s_prev.mean(dim=1)))\n\n            s_prev = self.encoder1(self.embedding(s_next.to('cuda:0')) + self.positional_encoding[:, :s_next.size(1), :]).to('cuda:1')\n\n        s_prev = self.encoder2(s_prev)\n        ret.append(self.fc(s_prev.mean(dim=1)))\n\n        return torch.cat(ret)\n\n","metadata":{"execution":{"iopub.status.busy":"2025-01-30T16:24:36.197290Z","iopub.execute_input":"2025-01-30T16:24:36.197647Z","iopub.status.idle":"2025-01-30T16:24:36.205617Z","shell.execute_reply.started":"2025-01-30T16:24:36.197621Z","shell.execute_reply":"2025-01-30T16:24:36.204269Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"setup = \"model = PipelineParallelTransformer(split_size=20)\"\nstmt = \"train(model)\"\npp_run_times = timeit.repeat(stmt, setup, number=1, repeat=num_repeat, globals=globals())\npp_mean, pp_std = np.mean(pp_run_times), np.std(pp_run_times)\n\nplot([mp_mean, rn_mean, pp_mean], [mp_std, rn_std, pp_std], ['Model Parallel', 'Single GPU', 'Pipeline Model Parallel'], 'mp_vs_rn_vs_pp.png')\n\nmeans = []\nstds = []\nsplit_sizes = [1, 3, 5, 8, 10, 12, 20, 40, 60]\n\nfor split_size in split_sizes:    \n    setup = f\"model = PipelineParallelTransformer(split_size={split_size})\"\n    pp_run_times = timeit.repeat(stmt, setup, number=1, repeat=num_repeat, globals=globals())\n    \n    \n    means.append(np.mean(pp_run_times))\n    stds.append(np.std(pp_run_times))\n    \nplt.style.use('ggplot')\nfig, ax = plt.subplots()\nax.plot(split_sizes, means)\nax.errorbar(split_sizes, means, yerr=stds, ecolor='red', fmt='ro')\nax.set_ylabel('Transformer Execution Time (Seconds)')\nax.set_xlabel('Pipeline Split Size')\nax.set_xticks(split_sizes)\nax.yaxis.grid(True)\nplt.tight_layout()\nplt.savefig(\"split_size_tradeoff.png\")\nplt.close(fig)\n","metadata":{"execution":{"iopub.status.busy":"2025-01-30T16:24:37.866563Z","iopub.execute_input":"2025-01-30T16:24:37.866997Z","iopub.status.idle":"2025-01-30T16:27:43.727000Z","shell.execute_reply.started":"2025-01-30T16:24:37.866963Z","shell.execute_reply":"2025-01-30T16:27:43.726023Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n<ipython-input-18-bbc73aa7653e>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x = torch.tensor(x).to(torch.int64)\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(\n<ipython-input-18-bbc73aa7653e>:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  x = torch.tensor(x).to(torch.int64)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}